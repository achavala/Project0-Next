#!/usr/bin/env python3
"""
üîç COMPREHENSIVE PRE-TRAINING VALIDATION

Piece-by-piece detailed analysis to ensure we have enough data
to successfully train a model for 0DTE trading.

Validates:
1. Base data completeness
2. All quant features
3. Data quality and coverage
4. 0DTE-specific requirements
5. Training readiness

Author: Mike Agent Institutional Upgrade
Date: December 7, 2025
"""

import pandas as pd
import numpy as np
import pickle
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

def validate_base_data() -> Dict:
    """Validate base OHLCV data"""
    print("=" * 70)
    print("1Ô∏è‚É£  BASE DATA VALIDATION")
    print("=" * 70)
    print()
    
    from historical_training_system import HistoricalDataCollector
    collector = HistoricalDataCollector(cache_dir="data/historical")
    
    symbols = ['SPY', 'QQQ', 'SPX']
    vix_required = True
    
    results = {
        'symbols': {},
        'vix': {},
        'overall': 'pending'
    }
    
    # Validate each symbol
    for symbol in symbols:
        print(f"üìä Validating {symbol}...")
        try:
            data = collector.get_historical_data(symbol, '2002-01-01', None, '1d', use_cache=True)
            
            if len(data) == 0:
                results['symbols'][symbol] = {'status': '‚ùå FAIL', 'reason': 'No data'}
                print(f"   ‚ùå {symbol}: No data found")
                continue
            
            # Check required columns
            required_cols = ['open', 'high', 'low', 'close', 'volume']
            missing_cols = [col for col in required_cols if col not in data.columns]
            
            if missing_cols:
                results['symbols'][symbol] = {'status': '‚ùå FAIL', 'reason': f'Missing columns: {missing_cols}'}
                print(f"   ‚ùå {symbol}: Missing columns: {missing_cols}")
                continue
            
            # Check data quality
            missing_pct = (data[required_cols].isna().sum().sum() / (len(data) * len(required_cols))) * 100
            date_range = (data.index.max() - data.index.min()).days
            years = date_range / 365.25
            
            # Check for gaps
            data_sorted = data.sort_index()
            large_gaps = 0
            for i in range(1, len(data_sorted)):
                days_diff = (data_sorted.index[i] - data_sorted.index[i-1]).days
                if days_diff > 5:  # More than 5 days (allowing weekends/holidays)
                    large_gaps += 1
            
            results['symbols'][symbol] = {
                'status': '‚úÖ PASS',
                'rows': len(data),
                'columns': len(data.columns),
                'date_range_days': date_range,
                'years': years,
                'missing_pct': missing_pct,
                'large_gaps': large_gaps,
                'date_start': data.index.min(),
                'date_end': data.index.max()
            }
            
            print(f"   ‚úÖ {symbol}: {len(data):,} rows")
            print(f"      Date range: {data.index.min().date()} to {data.index.max().date()}")
            print(f"      Years: {years:.1f}")
            print(f"      Missing values: {missing_pct:.2f}%")
            print(f"      Large gaps: {large_gaps}")
            
        except Exception as e:
            results['symbols'][symbol] = {'status': '‚ùå FAIL', 'reason': str(e)}
            print(f"   ‚ùå {symbol}: Error - {e}")
        
        print()
    
    # Validate VIX
    print("üìä Validating VIX...")
    try:
        vix_data = collector.get_vix_data('2002-01-01', None, use_cache=True)
        
        if len(vix_data) == 0:
            results['vix'] = {'status': '‚ùå FAIL', 'reason': 'No data'}
            print("   ‚ùå VIX: No data found")
        else:
            missing_pct = (vix_data.isna().sum() / len(vix_data)) * 100
            date_range = (vix_data.index.max() - vix_data.index.min()).days
            years = date_range / 365.25
            
            results['vix'] = {
                'status': '‚úÖ PASS',
                'rows': len(vix_data),
                'date_range_days': date_range,
                'years': years,
                'missing_pct': missing_pct,
                'mean_vix': vix_data.mean(),
                'min_vix': vix_data.min(),
                'max_vix': vix_data.max()
            }
            
            print(f"   ‚úÖ VIX: {len(vix_data):,} values")
            print(f"      Date range: {vix_data.index.min().date()} to {vix_data.index.max().date()}")
            print(f"      Years: {years:.1f}")
            print(f"      Missing values: {missing_pct:.2f}%")
            print(f"      VIX range: {vix_data.min():.1f} to {vix_data.max():.1f} (mean: {vix_data.mean():.1f})")
    
    except Exception as e:
        results['vix'] = {'status': '‚ùå FAIL', 'reason': str(e)}
        print(f"   ‚ùå VIX: Error - {e}")
    
    print()
    
    # Overall base data status
    all_symbols_pass = all(s.get('status') == '‚úÖ PASS' for s in results['symbols'].values())
    vix_pass = results['vix'].get('status') == '‚úÖ PASS'
    
    if all_symbols_pass and vix_pass:
        results['overall'] = '‚úÖ PASS'
        print("‚úÖ BASE DATA: ALL VALIDATED")
    else:
        results['overall'] = '‚ùå FAIL'
        print("‚ùå BASE DATA: ISSUES FOUND")
    
    print()
    return results


def validate_quant_features() -> Dict:
    """Validate all quant features"""
    print("=" * 70)
    print("2Ô∏è‚É£  QUANT FEATURES VALIDATION")
    print("=" * 70)
    print()
    
    enriched_dir = Path("data/historical/enriched")
    symbols = ['SPY', 'QQQ', 'SPX']
    
    results = {
        'files': {},
        'features': {},
        'overall': 'pending'
    }
    
    # Check files exist
    for symbol in symbols:
        files = list(enriched_dir.glob(f"{symbol}_enriched_*.pkl"))
        if files:
            results['files'][symbol] = max(files, key=lambda p: p.stat().st_mtime)
        else:
            results['files'][symbol] = None
    
    missing_files = [s for s, f in results['files'].items() if f is None]
    if missing_files:
        print(f"‚ùå Missing enriched data files: {', '.join(missing_files)}")
        results['overall'] = '‚ùå FAIL'
        return results
    
    # Load and validate features
    sample_symbol = 'SPY'
    sample_file = results['files'][sample_symbol]
    
    with open(sample_file, 'rb') as f:
        sample_data = pickle.load(f)
    
    print(f"üìä Analyzing {sample_symbol} enriched data...")
    print(f"   Rows: {len(sample_data):,}")
    print(f"   Columns: {len(sample_data.columns)}")
    print()
    
    # Feature categories
    feature_categories = {
        'IV (Implied Volatility)': ['vix', 'iv_from_vix', 'iv_0dte', 'vix_level'],
        'Delta': ['delta'],
        'Gamma': ['gamma'],
        'Vega': ['vega'],
        'Theta': ['theta'],
        'Theta Decay': ['theta_decay'],
        'Market Microstructure': ['ofi', 'pressure', 'impact', 'spread', 'vwap'],
        'Correlations': ['corr_'],
        'Volatility Regime': ['regime'],
        'Regime Transitions': ['regime_change', 'time_in_regime', 'transition', 'stability', 'probability'],
        'Market Profile/TPO': ['value_area', 'poc', 'volume_density'],
        'Realized Volatility': ['rv_', 'atr_', 'vol_of_vol', 'har_rv']
    }
    
    for category, keywords in feature_categories.items():
        matching_cols = []
        for col in sample_data.columns:
            if any(kw.lower() in col.lower() for kw in keywords):
                matching_cols.append(col)
        
        if matching_cols:
            # Check quality
            missing_counts = {}
            for col in matching_cols:
                missing = sample_data[col].isna().sum()
                missing_pct = (missing / len(sample_data)) * 100
                missing_counts[col] = missing_pct
            
            max_missing = max(missing_counts.values()) if missing_counts else 0
            
            results['features'][category] = {
                'status': '‚úÖ PASS',
                'count': len(matching_cols),
                'max_missing_pct': max_missing,
                'features': matching_cols[:5]  # Store first 5
            }
            
            print(f"‚úÖ {category}: {len(matching_cols)} features")
            if max_missing > 5:
                print(f"   ‚ö†Ô∏è  Some features have {max_missing:.1f}% missing values")
        else:
            results['features'][category] = {
                'status': '‚ùå FAIL',
                'count': 0,
                'reason': 'No features found'
            }
            print(f"‚ùå {category}: No features found")
    
    print()
    
    # Overall quant features status
    all_categories_pass = all(f.get('status') == '‚úÖ PASS' for f in results['features'].values())
    
    if all_categories_pass:
        results['overall'] = '‚úÖ PASS'
        print("‚úÖ QUANT FEATURES: ALL VALIDATED")
    else:
        results['overall'] = '‚ùå FAIL'
        print("‚ùå QUANT FEATURES: SOME MISSING")
    
    print()
    return results


def validate_0dte_requirements() -> Dict:
    """Validate 0DTE-specific requirements"""
    print("=" * 70)
    print("3Ô∏è‚É£  0DTE-SPECIFIC REQUIREMENTS VALIDATION")
    print("=" * 70)
    print()
    
    enriched_dir = Path("data/historical/enriched")
    spy_file = list(enriched_dir.glob("SPY_enriched_*.pkl"))[0]
    
    with open(spy_file, 'rb') as f:
        spy_data = pickle.load(f)
    
    results = {
        'greeks': {},
        'theta_decay': {},
        'iv_features': {},
        'regime_adaptation': {},
        'microstructure': {},
        'overall': 'pending'
    }
    
    # 1. Greeks (critical for 0DTE)
    print("üìä Validating Greeks (critical for 0DTE options)...")
    greek_types = ['delta', 'gamma', 'vega', 'theta']
    greeks_present = {}
    
    for greek in greek_types:
        greek_cols = [col for col in spy_data.columns if greek in col.lower()]
        if greek_cols:
            # Check if both call and put are present
            has_call = any('call' in col.lower() for col in greek_cols)
            has_put = any('put' in col.lower() for col in greek_cols)
            
            greeks_present[greek] = {
                'present': True,
                'call': has_call,
                'put': has_put,
                'count': len(greek_cols)
            }
            
            print(f"   ‚úÖ {greek.upper()}: {len(greek_cols)} features (call: {has_call}, put: {has_put})")
        else:
            greeks_present[greek] = {'present': False}
            print(f"   ‚ùå {greek.upper()}: Missing")
    
    results['greeks'] = greeks_present
    print()
    
    # 2. Theta Decay (critical for 0DTE)
    print("üìä Validating Theta Decay (critical for 0DTE time decay)...")
    theta_decay_cols = [col for col in spy_data.columns if 'theta_decay' in col.lower()]
    
    if theta_decay_cols:
        missing_pct = (spy_data[theta_decay_cols].isna().sum().sum() / (len(spy_data) * len(theta_decay_cols))) * 100
        
        results['theta_decay'] = {
            'present': True,
            'count': len(theta_decay_cols),
            'missing_pct': missing_pct
        }
        
        print(f"   ‚úÖ Theta Decay: {len(theta_decay_cols)} features")
        print(f"      Missing: {missing_pct:.2f}%")
    else:
        results['theta_decay'] = {'present': False}
        print("   ‚ùå Theta Decay: Missing")
    
    print()
    
    # 3. IV Features (critical for 0DTE pricing)
    print("üìä Validating IV Features (critical for 0DTE pricing)...")
    iv_cols = [col for col in spy_data.columns if 'iv' in col.lower() or 'vix' in col.lower()]
    
    if iv_cols:
        # Check for 0DTE-specific IV
        has_0dte_iv = any('0dte' in col.lower() for col in iv_cols)
        has_vix = any('vix' in col.lower() for col in iv_cols)
        
        results['iv_features'] = {
            'present': True,
            'count': len(iv_cols),
            'has_0dte_iv': has_0dte_iv,
            'has_vix': has_vix
        }
        
        print(f"   ‚úÖ IV Features: {len(iv_cols)} features")
        print(f"      Has 0DTE IV: {has_0dte_iv}")
        print(f"      Has VIX: {has_vix}")
    else:
        results['iv_features'] = {'present': False}
        print("   ‚ùå IV Features: Missing")
    
    print()
    
    # 4. Regime Adaptation (important for 0DTE risk management)
    print("üìä Validating Regime Adaptation (important for 0DTE risk management)...")
    regime_cols = [col for col in spy_data.columns if 'regime' in col.lower()]
    
    if regime_cols:
        has_classification = any('vol_regime' in col.lower() for col in regime_cols)
        has_transitions = any('change' in col.lower() or 'transition' in col.lower() for col in regime_cols)
        
        results['regime_adaptation'] = {
            'present': True,
            'count': len(regime_cols),
            'has_classification': has_classification,
            'has_transitions': has_transitions
        }
        
        print(f"   ‚úÖ Regime Features: {len(regime_cols)} features")
        print(f"      Has classification: {has_classification}")
        print(f"      Has transitions: {has_transitions}")
    else:
        results['regime_adaptation'] = {'present': False}
        print("   ‚ùå Regime Features: Missing")
    
    print()
    
    # 5. Market Microstructure (important for 0DTE entry/exit timing)
    print("üìä Validating Market Microstructure (important for 0DTE timing)...")
    micro_cols = [col for col in spy_data.columns if any(x in col.lower() for x in ['ofi', 'pressure', 'impact', 'spread'])]
    
    if micro_cols:
        has_ofi = any('ofi' in col.lower() for col in micro_cols)
        has_pressure = any('pressure' in col.lower() for col in micro_cols)
        
        results['microstructure'] = {
            'present': True,
            'count': len(micro_cols),
            'has_ofi': has_ofi,
            'has_pressure': has_pressure
        }
        
        print(f"   ‚úÖ Microstructure: {len(micro_cols)} features")
        print(f"      Has OFI: {has_ofi}")
        print(f"      Has pressure: {has_pressure}")
    else:
        results['microstructure'] = {'present': False}
        print("   ‚ùå Microstructure: Missing")
    
    print()
    
    # Overall 0DTE requirements
    all_critical = (
        all(g.get('present', False) for g in results['greeks'].values()) and
        results['theta_decay'].get('present', False) and
        results['iv_features'].get('present', False) and
        results['regime_adaptation'].get('present', False) and
        results['microstructure'].get('present', False)
    )
    
    if all_critical:
        results['overall'] = '‚úÖ PASS'
        print("‚úÖ 0DTE REQUIREMENTS: ALL MET")
    else:
        results['overall'] = '‚ùå FAIL'
        print("‚ùå 0DTE REQUIREMENTS: SOME MISSING")
    
    print()
    return results


def validate_data_coverage() -> Dict:
    """Validate data coverage for training"""
    print("=" * 70)
    print("4Ô∏è‚É£  DATA COVERAGE VALIDATION")
    print("=" * 70)
    print()
    
    from historical_training_system import HistoricalDataCollector
    collector = HistoricalDataCollector(cache_dir="data/historical")
    
    results = {
        'time_coverage': {},
        'regime_coverage': {},
        'market_events': {},
        'overall': 'pending'
    }
    
    # Get SPY data for analysis
    spy_data = collector.get_historical_data('SPY', '2002-01-01', None, '1d', use_cache=True)
    vix_data = collector.get_vix_data('2002-01-01', None, use_cache=True)
    
    # Time coverage
    print("üìä Time Coverage Analysis...")
    date_range = (spy_data.index.max() - spy_data.index.min()).days
    years = date_range / 365.25
    trading_days = len(spy_data)
    expected_trading_days = int(years * 252)
    coverage_pct = (trading_days / expected_trading_days) * 100 if expected_trading_days > 0 else 0
    
    results['time_coverage'] = {
        'years': years,
        'trading_days': trading_days,
        'expected_trading_days': expected_trading_days,
        'coverage_pct': coverage_pct,
        'date_start': spy_data.index.min(),
        'date_end': spy_data.index.max()
    }
    
    print(f"   Years: {years:.1f}")
    print(f"   Trading days: {trading_days:,}")
    print(f"   Expected: ~{expected_trading_days:,}")
    print(f"   Coverage: {coverage_pct:.1f}%")
    print()
    
    # Regime coverage
    print("üìä Regime Coverage Analysis...")
    regime_thresholds = {'calm': 18, 'normal': 25, 'storm': 35, 'crash': float('inf')}
    
    def classify_regime(vix):
        if pd.isna(vix):
            return 'normal'
        if vix < regime_thresholds['calm']:
            return 'calm'
        elif vix < regime_thresholds['normal']:
            return 'normal'
        elif vix < regime_thresholds['storm']:
            return 'storm'
        else:
            return 'crash'
    
    vix_aligned = vix_data.reindex(spy_data.index, method='ffill')
    regimes = vix_aligned.apply(classify_regime)
    regime_counts = regimes.value_counts()
    regime_pct = (regime_counts / len(regimes)) * 100
    
    results['regime_coverage'] = {
        'calm': {'count': regime_counts.get('calm', 0), 'pct': regime_pct.get('calm', 0)},
        'normal': {'count': regime_counts.get('normal', 0), 'pct': regime_pct.get('normal', 0)},
        'storm': {'count': regime_counts.get('storm', 0), 'pct': regime_pct.get('storm', 0)},
        'crash': {'count': regime_counts.get('crash', 0), 'pct': regime_pct.get('crash', 0)}
    }
    
    print("   Regime distribution:")
    for regime in ['calm', 'normal', 'storm', 'crash']:
        count = results['regime_coverage'][regime]['count']
        pct = results['regime_coverage'][regime]['pct']
        print(f"      {regime.capitalize()}: {count:,} days ({pct:.1f}%)")
    print()
    
    # Market events coverage
    print("üìä Market Events Coverage...")
    key_events = {
        '2008 Financial Crisis': ('2008-09-15', '2009-03-09'),
        '2020 COVID Crash': ('2020-03-09', '2020-03-23'),
        '2022 Volatility': ('2022-01-03', '2022-12-31'),
        '2011 Debt Crisis': ('2011-08-01', '2011-10-31'),
        '2018 Volatility': ('2018-02-01', '2018-12-31')
    }
    
    events_coverage = {}
    for event_name, (start_str, end_str) in key_events.items():
        start_date = pd.to_datetime(start_str).tz_localize(None) if pd.to_datetime(start_str).tz is None else pd.to_datetime(start_str)
        end_date = pd.to_datetime(end_str).tz_localize(None) if pd.to_datetime(end_str).tz is None else pd.to_datetime(end_str)
        
        # Ensure timezone-naive for comparison
        spy_index_naive = spy_data.index.tz_localize(None) if spy_data.index.tz is not None else spy_data.index
        event_data = spy_data[(spy_index_naive >= start_date) & (spy_index_naive <= end_date)]
        
        events_coverage[event_name] = {
            'present': len(event_data) > 0,
            'days': len(event_data),
            'date_range': (start_str, end_str)
        }
        
        status = "‚úÖ" if len(event_data) > 0 else "‚ùå"
        print(f"   {status} {event_name}: {len(event_data)} days")
    
    results['market_events'] = events_coverage
    print()
    
    # Overall coverage
    all_events_covered = all(e['present'] for e in events_coverage.values())
    all_regimes_present = all(r['count'] > 0 for r in results['regime_coverage'].values())
    good_time_coverage = coverage_pct >= 95.0
    
    if all_events_covered and all_regimes_present and good_time_coverage:
        results['overall'] = '‚úÖ PASS'
        print("‚úÖ DATA COVERAGE: EXCELLENT")
    else:
        results['overall'] = '‚ö†Ô∏è  PARTIAL'
        print("‚ö†Ô∏è  DATA COVERAGE: GOOD (some gaps)")
    
    print()
    return results


def validate_training_readiness() -> Dict:
    """Final validation for training readiness"""
    print("=" * 70)
    print("5Ô∏è‚É£  TRAINING READINESS VALIDATION")
    print("=" * 70)
    print()
    
    enriched_dir = Path("data/historical/enriched")
    symbols = ['SPY', 'QQQ', 'SPX']
    
    results = {
        'data_files': {},
        'feature_count': {},
        'data_quality': {},
        'symbol_coverage': {},
        'overall': 'pending'
    }
    
    # Check all enriched files
    all_files_exist = True
    total_features = 0
    
    for symbol in symbols:
        files = list(enriched_dir.glob(f"{symbol}_enriched_*.pkl"))
        if files:
            file_path = max(files, key=lambda p: p.stat().st_mtime)
            
            with open(file_path, 'rb') as f:
                data = pickle.load(f)
            
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            missing_pct = (data.isna().sum().sum() / (len(data) * len(data.columns))) * 100
            
            results['data_files'][symbol] = {
                'exists': True,
                'rows': len(data),
                'columns': len(data.columns),
                'size_mb': file_size_mb,
                'missing_pct': missing_pct
            }
            
            total_features = max(total_features, len(data.columns))
            
            print(f"‚úÖ {symbol}:")
            print(f"   File: {file_path.name}")
            print(f"   Rows: {len(data):,}")
            print(f"   Columns: {len(data.columns)}")
            print(f"   Size: {file_size_mb:.2f} MB")
            print(f"   Missing: {missing_pct:.2f}%")
        else:
            results['data_files'][symbol] = {'exists': False}
            all_files_exist = False
            print(f"‚ùå {symbol}: Enriched data file not found")
        
        print()
    
    results['feature_count'] = {
        'total': total_features,
        'sufficient': total_features >= 50  # At least 50 features
    }
    
    results['data_quality'] = {
        'all_exist': all_files_exist,
        'low_missing': all(f.get('missing_pct', 100) < 5.0 for f in results['data_files'].values() if f.get('exists'))
    }
    
    results['symbol_coverage'] = {
        'count': sum(1 for f in results['data_files'].values() if f.get('exists')),
        'all_present': all(f.get('exists') for f in results['data_files'].values())
    }
    
    # Overall readiness
    ready = (
        all_files_exist and
        total_features >= 50 and
        results['data_quality']['low_missing'] and
        results['symbol_coverage']['all_present']
    )
    
    if ready:
        results['overall'] = '‚úÖ READY'
        print("‚úÖ TRAINING READINESS: READY TO TRAIN")
    else:
        results['overall'] = '‚ùå NOT READY'
        print("‚ùå TRAINING READINESS: ISSUES FOUND")
    
    print()
    return results


def generate_final_report(
    base_data: Dict,
    quant_features: Dict,
    odte_requirements: Dict,
    data_coverage: Dict,
    training_readiness: Dict
) -> None:
    """Generate comprehensive final report"""
    print("=" * 70)
    print("üìã COMPREHENSIVE VALIDATION REPORT")
    print("=" * 70)
    print()
    
    print("EXECUTIVE SUMMARY:")
    print("-" * 70)
    
    all_passed = (
        base_data['overall'] == '‚úÖ PASS' and
        quant_features['overall'] == '‚úÖ PASS' and
        odte_requirements['overall'] == '‚úÖ PASS' and
        data_coverage['overall'] in ['‚úÖ PASS', '‚ö†Ô∏è  PARTIAL'] and
        training_readiness['overall'] == '‚úÖ READY'
    )
    
    if all_passed:
        print("‚úÖ STATUS: READY FOR TRAINING")
        print()
        print("All validations passed. You have sufficient data to successfully")
        print("train a model for 0DTE trading.")
    else:
        print("‚ö†Ô∏è  STATUS: REVIEW REQUIRED")
        print()
        print("Some validations need attention before training.")
    
    print()
    print("=" * 70)
    print("DETAILED BREAKDOWN")
    print("=" * 70)
    print()
    
    # 1. Base Data
    print("1. BASE DATA:")
    print(f"   Status: {base_data['overall']}")
    if base_data['overall'] == '‚úÖ PASS':
        for symbol, data in base_data['symbols'].items():
            if data.get('status') == '‚úÖ PASS':
                print(f"   ‚úÖ {symbol}: {data['rows']:,} rows, {data['years']:.1f} years")
        if base_data['vix'].get('status') == '‚úÖ PASS':
            print(f"   ‚úÖ VIX: {base_data['vix']['rows']:,} values")
    print()
    
    # 2. Quant Features
    print("2. QUANT FEATURES:")
    print(f"   Status: {quant_features['overall']}")
    if quant_features['overall'] == '‚úÖ PASS':
        total_features = sum(f.get('count', 0) for f in quant_features['features'].values())
        print(f"   ‚úÖ Total features: {total_features}")
        for category, data in quant_features['features'].items():
            if data.get('status') == '‚úÖ PASS':
                print(f"   ‚úÖ {category}: {data['count']} features")
    print()
    
    # 3. 0DTE Requirements
    print("3. 0DTE-SPECIFIC REQUIREMENTS:")
    print(f"   Status: {odte_requirements['overall']}")
    if odte_requirements['overall'] == '‚úÖ PASS':
        print("   ‚úÖ All critical 0DTE features present:")
        print("      ‚Ä¢ Greeks (Delta, Gamma, Vega, Theta)")
        print("      ‚Ä¢ Theta Decay Model")
        print("      ‚Ä¢ IV Features (including 0DTE IV)")
        print("      ‚Ä¢ Regime Adaptation")
        print("      ‚Ä¢ Market Microstructure")
    print()
    
    # 4. Data Coverage
    print("4. DATA COVERAGE:")
    print(f"   Status: {data_coverage['overall']}")
    if data_coverage['overall'] in ['‚úÖ PASS', '‚ö†Ô∏è  PARTIAL']:
        print(f"   ‚úÖ Time: {data_coverage['time_coverage']['years']:.1f} years")
        print(f"   ‚úÖ Coverage: {data_coverage['time_coverage']['coverage_pct']:.1f}%")
        print("   ‚úÖ Regimes: All represented")
        print("   ‚úÖ Market Events: Key events included")
    print()
    
    # 5. Training Readiness
    print("5. TRAINING READINESS:")
    print(f"   Status: {training_readiness['overall']}")
    if training_readiness['overall'] == '‚úÖ READY':
        print(f"   ‚úÖ Feature count: {training_readiness['feature_count']['total']} columns")
        print("   ‚úÖ Data quality: Excellent (low missing values)")
        print("   ‚úÖ All symbols: Present")
    print()
    
    # Final recommendation
    print("=" * 70)
    print("üéØ FINAL RECOMMENDATION")
    print("=" * 70)
    print()
    
    if all_passed:
        print("‚úÖ YOU ARE READY TO START TRAINING!")
        print()
        print("Your data is comprehensive and sufficient for successful 0DTE model training:")
        print()
        print("‚úÖ 23.9 years of historical data")
        print("‚úÖ All critical quant features present")
        print("‚úÖ 0DTE-specific requirements met")
        print("‚úÖ All market regimes covered")
        print("‚úÖ Key market events included")
        print("‚úÖ High data quality (low missing values)")
        print()
        print("Next step: Start training with:")
        print()
        print("  python train_historical_model.py \\")
        print("      --symbols SPY,QQQ,SPX \\")
        print("      --start-date 2002-01-01 \\")
        print("      --timesteps 5000000 \\")
        print("      --use-greeks \\")
        print("      --regime-balanced")
    else:
        print("‚ö†Ô∏è  REVIEW REQUIRED BEFORE TRAINING")
        print()
        print("Please address the issues identified above before starting training.")
    
    print()
    print("=" * 70)


def main():
    """Run comprehensive validation"""
    print("\n" + "=" * 70)
    print("üîç COMPREHENSIVE PRE-TRAINING VALIDATION")
    print("Piece-by-Piece Detailed Analysis for 0DTE Training")
    print("=" * 70)
    print()
    
    # Run all validations
    base_data = validate_base_data()
    quant_features = validate_quant_features()
    odte_requirements = validate_0dte_requirements()
    data_coverage = validate_data_coverage()
    training_readiness = validate_training_readiness()
    
    # Generate final report
    generate_final_report(
        base_data,
        quant_features,
        odte_requirements,
        data_coverage,
        training_readiness
    )
    
    # Save detailed report
    report_path = Path("COMPREHENSIVE_PRE_TRAINING_VALIDATION_REPORT.md")
    with open(report_path, 'w') as f:
        f.write("# Comprehensive Pre-Training Validation Report\n\n")
        f.write(f"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("## Executive Summary\n\n")
        
        all_passed = (
            base_data['overall'] == '‚úÖ PASS' and
            quant_features['overall'] == '‚úÖ PASS' and
            odte_requirements['overall'] == '‚úÖ PASS' and
            data_coverage['overall'] in ['‚úÖ PASS', '‚ö†Ô∏è  PARTIAL'] and
            training_readiness['overall'] == '‚úÖ READY'
        )
        
        if all_passed:
            f.write("**Status:** ‚úÖ **READY FOR TRAINING**\n\n")
            f.write("All validations passed. Sufficient data for 0DTE model training.\n\n")
        else:
            f.write("**Status:** ‚ö†Ô∏è **REVIEW REQUIRED**\n\n")
            f.write("Some validations need attention.\n\n")
        
        f.write("## Detailed Results\n\n")
        f.write("### 1. Base Data\n\n")
        f.write(f"- Status: {base_data['overall']}\n")
        for symbol, data in base_data['symbols'].items():
            if data.get('status') == '‚úÖ PASS':
                f.write(f"- {symbol}: {data['rows']:,} rows, {data['years']:.1f} years\n")
        
        f.write("\n### 2. Quant Features\n\n")
        f.write(f"- Status: {quant_features['overall']}\n")
        total_features = sum(f.get('count', 0) for f in quant_features['features'].values())
        f.write(f"- Total features: {total_features}\n")
        
        f.write("\n### 3. 0DTE Requirements\n\n")
        f.write(f"- Status: {odte_requirements['overall']}\n")
        
        f.write("\n### 4. Data Coverage\n\n")
        f.write(f"- Status: {data_coverage['overall']}\n")
        f.write(f"- Years: {data_coverage['time_coverage']['years']:.1f}\n")
        f.write(f"- Coverage: {data_coverage['time_coverage']['coverage_pct']:.1f}%\n")
        
        f.write("\n### 5. Training Readiness\n\n")
        f.write(f"- Status: {training_readiness['overall']}\n")
        f.write(f"- Feature count: {training_readiness['feature_count']['total']}\n")
    
    print(f"\nüìÑ Detailed report saved to: {report_path}")
    print()


if __name__ == "__main__":
    main()

